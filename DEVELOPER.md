# CineFlow AI å¼€å‘æŒ‡å—

> é¢å‘å¼€å‘è€…çš„å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

---

## ğŸ“¦ å¼€å‘ç¯å¢ƒè®¾ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv .venv
source .venv/bin/activate  # macOS/Linux
.venv\Scripts\activate   # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### 2. é…ç½® API å¯†é’¥ï¼ˆå¼€å‘æ¨¡å¼ï¼‰

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆæœ¬åœ°å¼€å‘æ–¹ä¾¿æµ‹è¯•ï¼‰
export QWEN_API_KEY="your-api-key"
export KIMI_API_KEY="your-api-key"
export GLM5_API_KEY="your-api-key"

# æˆ–åœ¨ config/llm.yaml ä¸­é…ç½®
```

### 3. è¿è¡Œåº”ç”¨

```bash
python3 app/main.py
# æˆ–
cineflow-ai
```

---

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

```bash
# åŸºç¡€æµ‹è¯•ï¼ˆä¸éœ€è¦ API å¯†é’¥ï¼‰
pytest tests/ -v

# å®Œæ•´æµ‹è¯•ï¼ˆåŒ…å«é›†æˆæµ‹è¯•ï¼Œéœ€è¦ API å¯†é’¥ï¼‰
pytest tests/ -v -m "integration"
```

### è¿è¡Œç‰¹å®šæµ‹è¯•

```bash
# ç‰ˆæœ¬æµ‹è¯•
pytest tests/test_version.py -v

# LLM æä¾›å•†æµ‹è¯•
pytest tests/test_llm_providers.py -v

# æ–‡æ¡ˆç”Ÿæˆå™¨æµ‹è¯•
pytest tests/test_script_generator.py -v

# é›†æˆæµ‹è¯•
pytest tests/test_integration.py -v -m "integration"
```

### æµ‹è¯•è¦†ç›–ç‡

```bash
# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=app --cov-report=html tests/

# æŸ¥çœ‹æŠ¥å‘Š
open htmlcov/index.html  # macOS
start htmlcov/index.html # Windows
```

---

## ğŸ“ ä»£ç ç»“æ„

```
CineAIStudio/
â”œâ”€â”€ app/                    # åº”ç”¨ä»£ç 
â”‚   â”œâ”€â”€ core/              # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ models/        # æ•°æ®æ¨¡å‹ (LLM, Script ç­‰)
â”‚   â”‚   â”œâ”€â”€ config.py      # é…ç½®ç®¡ç†
â”‚   â”‚   â””â”€â”€ logger.py      # æ—¥å¿—ç®¡ç†
â”‚   â”œâ”€â”€ services/          # æœåŠ¡å±‚
â”‚   â”‚   â”œâ”€â”€ ai/           # AI æœåŠ¡ (LLM, æ–‡æ¡ˆç”Ÿæˆ)
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_manager.py       # LLM ç®¡ç†å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ script_generator.py  # æ–‡æ¡ˆç”Ÿæˆå™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ base_LLM_provider.py # LLM æŠ½è±¡æ¥å£
â”‚   â”‚   â”‚   â””â”€â”€ providers/            # LLM æä¾›å•†
â”‚   â”‚   â”‚       â”œâ”€â”€ qwen.py
â”‚   â”‚   â”‚       â”œâ”€â”€ kimi.py
â”‚   â”‚   â”‚       â””â”€â”€ glm5.py
â”‚   â”‚   â”œâ”€â”€ video/        # è§†é¢‘æœåŠ¡
â”‚   â”‚   â””â”€â”€ audio/        # éŸ³é¢‘æœåŠ¡
â”‚   â””â”€â”€ utils/            # å·¥å…·æ¨¡å—
â”‚       â””â”€â”€ version.py    # ç‰ˆæœ¬ç®¡ç†
â”œâ”€â”€ config/               # é…ç½®æ–‡ä»¶
â”œâ”€â”€ tests/                # æµ‹è¯•ä»£ç 
â”œâ”€â”€ docs/                 # æ–‡æ¡£
â”œâ”€â”€ pyproject.toml        # é¡¹ç›®é…ç½® (Python 3.12+)
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â””â”€â”€ main.py               # åº”ç”¨å…¥å£
```

---

## ğŸš€ æ·»åŠ æ–°çš„ LLM æä¾›å•†

### æ­¥éª¤ 1: åˆ›å»ºæä¾›å•†ç±»

åœ¨ `app/services/ai/providers/` åˆ›å»ºæ–°æ–‡ä»¶ï¼š

```python
# app/services/ai/providers/custom_provider.py

from app.services.ai.base_LLM_provider import BaseLLMProvider
from app.core.models.llm_models import LLMRequest, LLMResponse

class CustomProvider(BaseLLMProvider):
    """è‡ªå®šä¹‰ LLM æä¾›å•†"""

    def __init__(self, api_key: str, base_url: str = None):
        super().__init__(api_key, base_url)
        self.models = {
            "custom-model": {
                "name": "custom-model",
                "max_tokens": 4096,
                "supports_vision": False
            }
        }
        self.default_model = "custom-model"

    async def complete(self, request: LLMRequest) -> LLMResponse:
        """å®ç°è¡¥å…¨é€»è¾‘"""
        # è°ƒç”¨ API
        # è¿”å› LLMResponse
        pass

    async def check_health(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        # æ£€æŸ¥ API å¯ç”¨æ€§
        # è¿”å› True/False
        pass
```

### æ­¥éª¤ 2: åœ¨ LLMManager ä¸­æ³¨å†Œ

ç¼–è¾‘ `config/llm.yaml`:

```yaml
LLM:
  custom:
    enabled: true
    api_key: "your-api-key"
    base_url: "https://api.custom.com/v1"
    model: "custom-model"
```

### æ­¥éª¤ 3: æ·»åŠ æµ‹è¯•

åˆ›å»º `tests/test_custom_provider.py`:

```python
from app.services.ai.providers.custom_provider import CustomProvider
from app.core.models.llm_models import LLMRequest

def test_custom_provider_init():
    provider = CustomProvider(api_key="test")
    assert "custom-model" in provider.models
    assert provider.default_model == "custom-model"

def test_custom_provider_request():
    provider = CustomProvider(api_key="test")
    request = LLMRequest(
        messages=[{"role": "user", "content": "Hello"}],
        model="custom-model"
    )
    response = await provider.complete(request)
    assert response.success is True
```

### æ­¥éª¤ 4: æ›´æ–°æ–‡æ¡£

æ›´æ–° `NATIVE-LLM-INTEGRATION.md` å’Œ `INSTALL.md`ã€‚

---

## ğŸ”„ Git å·¥ä½œæµ

### åˆ†æ”¯ç­–ç•¥

```
main (ä¸»åˆ†æ”¯ï¼Œç¨³å®šç‰ˆæœ¬)
â”œâ”€â”€ develop (å¼€å‘åˆ†æ”¯)
â””â”€â”€ feature/* (åŠŸèƒ½åˆ†æ”¯)
```

### æäº¤è§„èŒƒ

```
ç±»å‹(èŒƒå›´): ç®€çŸ­æè¿°

[å¯é€‰çš„è¯¦ç»†è¯´æ˜]

[å¯é€‰çš„å…³è” Issues]

ç±»å‹:
- âœ¨ feature: æ–°åŠŸèƒ½
- ğŸ› fix: ä¿®å¤ Bug
- ğŸ“ docs: æ–‡æ¡£æ›´æ–°
- ğŸ¨ style: ä»£ç æ ¼å¼
- â™»ï¸ refactor: é‡æ„
- âœ… test: æµ‹è¯•ç›¸å…³
- âš¡ perf: æ€§èƒ½ä¼˜åŒ–
```

### æäº¤ç¤ºä¾‹

```bash
git add .
git commit -m "âœ¨ æ·»åŠ è‡ªå®šä¹‰ LLM æä¾›å•†æ”¯æŒ

- å®ç° CustomProvider
- æ·»åŠ é…ç½®é€‰é¡¹
- å¢åŠ å•å…ƒæµ‹è¯•

Fixes #123"
```

### æ¨é€åˆ° GitHub

```bash
git push origin main
# æˆ–ä½¿ç”¨ SSH é…ç½®
GIT_SSH_COMMAND="ssh -i ~/.ssh/man_pipeline" git push origin main
```

---

## ğŸ“‹ å¾…åŠäº‹é¡¹

### v2.0.0 æ­£å¼ç‰ˆ

- [ ] å®Œæˆé›†æˆæµ‹è¯•
- [ ] æ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹
- [ ] æ€§èƒ½ä¼˜åŒ–
- [ ] æ›´ CHANGELOG.md
- [ ] å‘å¸ƒ v2.0.0 æ­£å¼ç‰ˆ

### v2.1.0

- [ ] æœ¬åœ° LLM æ”¯æŒ (Ollama)
- [ ] æ›´å¤šè¯­éŸ³é£æ ¼
- [ ] æ‰¹é‡å¤„ç†åŠŸèƒ½

---

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q: æµ‹è¯•å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹ï¼š

1. æ˜¯å¦é…ç½®äº† API å¯†é’¥ï¼Ÿ
```bash
export QWEN_API_KEY="your-key"
```

2. æ˜¯å¦å®‰è£…äº†æ‰€æœ‰ä¾èµ–ï¼Ÿ
```bash
pip install -r requirements.txt
```

3. æ˜¯å¦å¯åŠ¨äº†è™šæ‹Ÿç¯å¢ƒï¼Ÿ
```bash
source .venv/bin/activate
```

### Q: å¦‚ä½•è°ƒè¯• LLM è°ƒç”¨ï¼Ÿ

**A**: å¯ç”¨è¯¦ç»†æ—¥å¿—ï¼š

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

### Q: å¦‚ä½•è´¡çŒ®ä»£ç ï¼Ÿ

**A**:

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯
3. ç¼–å†™æµ‹è¯•
4. æäº¤ Pull Request

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

- [INSTALL.md](INSTALL.md) - å®‰è£…æŒ‡å—
- [TECH-STACK.md](TECH-STACK.md) - æŠ€æœ¯æ ˆè¯´æ˜
- [NATIVE-LLM-INTEGRATION.md](NATIVE-LLM-INTEGRATION.md) - LLM é›†æˆè®¾è®¡
- [VERSION-UNIFICATION.md](VERSION-UNIFICATION.md) - ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

---

**æœ€åæ›´æ–°**: 2026-02-14
**ç‰ˆæœ¬**: v2.0.0-rc.1
