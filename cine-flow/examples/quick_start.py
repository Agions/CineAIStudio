#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ClipFlow å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ ClipFlow çš„æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
from app.core.config_manager import get_config_manager
from app.core.exceptions import LLMError, ConfigError
from app.services.ai.llm_manager import LLMManager
from app.services.ai.providers.qwen import QwenProvider
from app.services.ai.providers.kimi import KimiProvider
from app.services.ai.cache import get_global_cache, with_retry, LLMRetryPolicy
from app.services.ai.script_generator import ScriptGenerator


async def example_1_basic_llm_call():
    """ç¤ºä¾‹ 1: åŸºæœ¬ LLM è°ƒç”¨"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 1: åŸºæœ¬ LLM è°ƒç”¨")
    print("="*50)

    # 1. åˆ›å»ºæä¾›å•†
    provider = QwenProvider(api_key="your-api-key")

    # 2. è°ƒç”¨ LLM
    from app.core.models.llm_models import LLMRequest
    request = LLMRequest(
        messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"}],
        model="qwen-plus",
        max_tokens=100
    )

    response = await provider.complete(request)

    if response.success:
        print(f"\nâœ… LLM å“åº”:")
        print(response.text)
    else:
        print(f"\nâŒ LLM è°ƒç”¨å¤±è´¥: {response.error}")


async def example_2_use_llm_manager():
    """ç¤ºä¾‹ 2: ä½¿ç”¨ LLM ç®¡ç†å™¨"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 2: ä½¿ç”¨ LLM ç®¡ç†å™¨")
    print("="*50)

    # 1. åˆ›å»ºå¤šä¸ªæä¾›å•†
    providers = {
        "qwen": QwenProvider(api_key="qwen-api-key"),
        "kimi": KimiProvider(api_key="kimi-api-key")
    }

    # 2. åˆ›å»ºç®¡ç†å™¨
    manager = LLMManager(providers=providers, default_provider="qwen")

    # 3. ä½¿ç”¨ç®¡ç†å™¨è°ƒç”¨
    from app.core.models.llm_models import LLMRequest
    request = LLMRequest(
        messages=[{"role": "user", "content": "å†™ä¸€ä¸ªç®€çŸ­çš„æ•…äº‹"}],
        max_tokens=200
    )

    # ä½¿ç”¨é»˜è®¤æä¾›å•†
    response = await manager.complete(request)

    if response.success:
        print(f"\nâœ… ä½¿ç”¨é»˜è®¤æä¾›å•† (qwen):")
        print(response.text[:100] + "...")

    # åˆ‡æ¢åˆ°å…¶ä»–æä¾›å•†
    response2 = await manager.complete(request, provider_name="kimi")

    if response2.success:
        print(f"\nâœ… ä½¿ç”¨æä¾›å•† (kimi):")
        print(response2.text[:100] + "...")


async def example_3_use_cache():
    """ç¤ºä¾‹ 3: ä½¿ç”¨ç¼“å­˜"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 3: ä½¿ç”¨ç¼“å­˜")
    print("="*50)

    # 1. è·å–å…¨å±€ç¼“å­˜
    cache = get_global_cache()

    # 2. ç¬¬ä¸€æ¬¡è°ƒç”¨ (æœªç¼“å­˜)
    messages = [{"role": "user", "content": "1+1 ç­‰äºå‡ ï¼Ÿ"}]
    cached_response = cache.get(messages, "qwen")

    if cached_response:
        print(f"\nâœ… ä»ç¼“å­˜è·å–: {cached_response}")
    else:
        print(f"\nğŸ”„ æœªç¼“å­˜ï¼Œéœ€è¦è°ƒç”¨ LLM")
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™… LLM
        simulated_response = "1+1=2"
        cache.set(messages, "qwen", simulated_response)
        print(f"âœ… å·²ç¼“å­˜å“åº”: {simulated_response}")

    # 3. ç¬¬äºŒæ¬¡è°ƒç”¨ (æœ‰ç¼“å­˜)
    cached_response = cache.get(messages, "qwen")

    if cached_response:
        print(f"\nâœ… ç¬¬äºŒæ¬¡ä»ç¼“å­˜è·å–: {cached_response}")

    # 4. æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡: {cache.get_stats()}")


async def example_4_use_retry():
    """ç¤ºä¾‹ 4: ä½¿ç”¨é‡è¯•æœºåˆ¶"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 4: ä½¿ç”¨é‡è¯•æœºåˆ¶")
    print("="*50)

    # 1. åˆ›å»ºé‡è¯•ç­–ç•¥
    policy = LLMRetryPolicy(max_retries=3, base_delay=1.0)

    # 2. å®šä¹‰éœ€è¦é‡è¯•çš„å‡½æ•°
    call_count = 0

    @with_retry(policy, exceptions=(ConnectionError, TimeoutError))
    async def call_unstable_api():
        global call_count
        call_count += 1

        print(f"\nğŸ”„ å°è¯•ç¬¬ {call_count} æ¬¡...")

        if call_count < 3:
            raise ConnectionError("ç½‘ç»œè¿æ¥å¤±è´¥")

        return "âœ… æˆåŠŸ!"

    # 3. è°ƒç”¨å‡½æ•°
    try:
        result = await call_unstable_api()
        print(f"\n{result}")
        print(f"æ€»å…±å°è¯•äº† {call_count} æ¬¡")
    except Exception as e:
        print(f"\nâŒ æ‰€æœ‰é‡è¯•å‡å¤±è´¥: {e}")


async def example_5_generate_script():
    """ç¤ºä¾‹ 5: ç”Ÿæˆè„šæœ¬"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 5: ç”Ÿæˆè„šæœ¬")
    print("="*50)

    # 1. åˆ›å»ºè„šæœ¬ç”Ÿæˆå™¨
    generator = ScriptGenerator(use_llm_manager=False)

    # 2. ä½¿ç”¨æœ¬åœ°æ¨¡å¼ç”Ÿæˆè„šæœ¬
    script = generator.generate_commentary(
        topic="åˆ†æã€Šæµæµªåœ°çƒã€‹çš„ç§‘å­¦è®¾å®š",
        duration=60,
        style="explainer"
    )

    print(f"\nğŸ“ ç”Ÿæˆçš„è„šæœ¬:")
    print(f"æ ‡é¢˜: {script.title}")
    print(f"å†…å®¹: {script.text}")
    print(f"æ®µè½æ•°: {len(script.segments)}")

    if script.segments:
        print(f"\nç¬¬ä¸€æ®µ: {script.segments[0].text}")


async def example_6_config_management():
    """ç¤ºä¾‹ 6: é…ç½®ç®¡ç†"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 6: é…ç½®ç®¡ç†")
    print("="*50)

    # 1. è·å–é…ç½®ç®¡ç†å™¨
    config_manager = get_config_manager()

    # 2. åŠ è½½é…ç½®
    config = config_manager.load_config()

    print(f"\nğŸ“‹ å½“å‰é…ç½®:")
    print(f"  é»˜è®¤æä¾›å•†: {config.default_provider}")
    print(f"  æ—¥å¿—çº§åˆ«: {config.log_level}")
    print(f"  ç¼“å­˜å¯ç”¨: {config.cache.enabled}")
    print(f"  æœ€å¤§é‡è¯•æ¬¡æ•°: {config.retry.max_retries}")

    # 3. è·å–ç‰¹å®šæä¾›å•†é…ç½®
    qwen_config = config_manager.get_llm_config("qwen")
    if qwen_config:
        print(f"\nğŸ¤– é€šä¹‰åƒé—®é…ç½®:")
        print(f"  å¯ç”¨: {qwen_config.enabled}")
        print(f"  æ¨¡å‹: {qwen_config.model}")

    # 4. ä¿®æ”¹é…ç½®
    print(f"\nâœ… é‡è¯•æ¬¡æ•°å·²è®¾ç½®")


def example_7_error_handling():
    """ç¤ºä¾‹ 7: é”™è¯¯å¤„ç†"""
    print("\n" + "="*50)
    print("ç¤ºä¾‹ 7: é”™è¯¯å¤„ç†")
    print("="*50)

    try:
        # æ¨¡æ‹Ÿ LLM é”™è¯¯
        raise LLMError(
            message="API è°ƒç”¨å¤±è´¥: rate limit exceeded",
            provider="qwen",
            model="qwen-plus"
        )
    except LLMError as e:
        print(f"\nâŒ æ•è·åˆ° LLM é”™è¯¯:")
        print(e)

    try:
        # æ¨¡æ‹Ÿé…ç½®é”™è¯¯
        raise ConfigError("API å¯†é’¥æœªè®¾ç½®", key="qwen.api_key")
    except ConfigError as e:
        print(f"\nâŒ æ•è·åˆ°é…ç½®é”™è¯¯:")
        print(e)


async def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "="*50)
    print("ClipFlow å¿«é€Ÿå¼€å§‹ç¤ºä¾‹")
    print("="*50)

    # è¿è¡Œç¤ºä¾‹ (æ³¨æ„: éœ€è¦çœŸå®çš„ API å¯†é’¥æ‰èƒ½å®Œæ•´è¿è¡Œ)
    await example_3_use_cache()
    await example_4_use_retry()
    await example_5_generate_script()
    await example_6_config_management()
    example_7_error_handling()

    print("\n" + "="*50)
    print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆ!")
    print("="*50)
    print("\nğŸ’¡ æç¤º:")
    print("  - ç¤ºä¾‹ 1, 2 éœ€è¦çœŸå®çš„ LLM API å¯†é’¥")
    print("  - ç¤ºä¾‹ 3, 4, 5, 6, 7 å¯ä»¥ç›´æ¥è¿è¡Œ")
    print("  - å®Œæ•´æ–‡æ¡£è¯·æŸ¥çœ‹: docs/ å’Œ examples/ ç›®å½•")
    print()


if __name__ == "__main__":
    asyncio.run(main())
