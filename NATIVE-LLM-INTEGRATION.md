# CineFlow AI å›½äº§ LLM é›†æˆæ–¹æ¡ˆ

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2026-02-14

---

## ğŸ“‹ éœ€æ±‚åˆ†æ

### å½“å‰é—®é¢˜

```
ç°çŠ¶: ä»…æ”¯æŒ OpenAI API
é—®é¢˜:
1. âŒ å›½äº§æ¨¡å‹æœªé›†æˆï¼ˆé€šä¹‰åƒé—®ã€Kimiã€GLM-5ï¼‰
2. âŒ API é”å®šï¼Œæ— æ³•åˆ‡æ¢
3. âŒ ä¸ç¬¦åˆå›½äº§åŒ–è¦æ±‚
4. âŒ æˆæœ¬é«˜ï¼Œé€Ÿåº¦æ…¢
```

### ç›®æ ‡

```
ç›®æ ‡: æ”¯æŒå¤šå›½äº§ LLMï¼Œæœ¬åœ°ä¼˜å…ˆ
è¦æ±‚:
1. âœ… é€šä¹‰åƒé—® Qwen 3
2. âœ… Kimi 2.5
3. âœ… æ™ºè°± GLM-5
4. âœ… ç™¾åº¦æ–‡å¿ƒ ERNIE 4.5
5. âœ… ç»Ÿä¸€æ¥å£ï¼Œæ˜“äºåˆ‡æ¢
6. âœ… æœ¬åœ°æ¨¡å‹æ”¯æŒï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### 1. æŠ½è±¡å±‚è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Application Layer               â”‚
â”‚    (ScriptGenerator, CommentaryMaker, ...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           LLM Manager Layer                 â”‚
â”‚     (LLMManager - ç»Ÿä¸€ç®¡ç†ï¼Œè‡ªåŠ¨åˆ‡æ¢)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Provider Interface               â”‚
â”‚      (BaseLLMProvider - æŠ½è±¡æ¥å£)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“             â†“             â†“
    QwenProvider   KimiProvider  ...
```

### 2. ç›®å½•ç»“æ„

```
app/services/ai/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ base_LLM_provider.py       # æŠ½è±¡åŸºç±»
â”œâ”€â”€ llm_manager.py             # LLM ç®¡ç†å™¨
â”œâ”€â”€ providers/                 # æä¾›å•†å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ openai.py              # OpenAI
â”‚   â”œâ”€â”€ qwen.py                # é€šä¹‰åƒé—®
â”‚   â”œâ”€â”€ kimi.py                # Kimi
â”‚   â”œâ”€â”€ glm5.py                # GLM-5
â”‚   â”œâ”€â”€ baidu.py               # ç™¾åº¦æ–‡å¿ƒ
â”‚   â””â”€â”€ local.py               # æœ¬åœ°æ¨¡å‹
â”œâ”€â”€ script_generator.py        # æ–‡æ¡ˆç”Ÿæˆå™¨ï¼ˆæ›´æ–°ï¼‰
â””â”€â”€ config.py                  # LLM é…ç½®
```

---

## ğŸ“ æ¥å£è®¾è®¡

### BaseLLMProvider

```python
"""
LLM æä¾›å•†æŠ½è±¡åŸºç±»
æ‰€æœ‰å…·ä½“æä¾›å•†å¿…é¡»å®ç°æ­¤æ¥å£
"""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from dataclasses import dataclass


@dataclass
class LLMRequest:
    """LLM è¯·æ±‚"""
    prompt: str                          # æç¤ºè¯
    system_prompt: str = ""               # ç³»ç»Ÿæç¤ºè¯
    model: str = "default"                # æ¨¡å‹åç§°
    max_tokens: int = 2000               # æœ€å¤§ç”Ÿæˆé•¿åº¦
    temperature: float = 0.7              # æ¸©åº¦å‚æ•°
    top_p: float = 0.9                   # Top-p å‚æ•°


@dataclass
class LLMResponse:
    """LLM å“åº”"""
    content: str                         # ç”Ÿæˆå†…å®¹
    model: str                           # ä½¿ç”¨çš„æ¨¡å‹
    tokens_used: int = 0                 # Token ä½¿ç”¨é‡
    finish_reason: str = "stop"          # ç»“æŸåŸå› 
    metadata: Dict[str, Any] = None      # å…ƒæ•°æ®

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class BaseLLMProvider(ABC):
    """
    LLM æä¾›å•†æŠ½è±¡åŸºç±»

    æ‰€æœ‰ LLM æä¾›å•†å¿…é¡»ç»§æ‰¿æ­¤ç±»å¹¶å®ç°æŠ½è±¡æ–¹æ³•
    """

    def __init__(self, api_key: str, base_url: str = ""):
        """
        åˆå§‹åŒ–æä¾›å•†

        Args:
            api_key: API å¯†é’¥
            base_url: API åŸºç¡€ URL
        """
        self.api_key = api_key
        self.base_url = base_url

    @abstractmethod
    def generate(self, request: LLMRequest) -> LLMResponse:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            request: LLM è¯·æ±‚

        Returns:
            LLM å“åº”

        Raises:
            ProviderError: æä¾›å•†é”™è¯¯
        """
        pass

    @abstractmethod
    def generate_batch(
        self,
        requests: List[LLMRequest],
    ) -> List[LLMResponse]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬

        Args:
            requests: LLM è¯·æ±‚åˆ—è¡¨

        Returns:
            LLM å“åº”åˆ—è¡¨
        """
        pass

    @abstractmethod
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

        Returns:
            æ¨¡å‹åç§°åˆ—è¡¨
        """
        pass

    @abstractmethod
    def get_model_info(self, model: str) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯

        Args:
            model: æ¨¡å‹åç§°

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        pass

    def health_check(self) -> bool:
        """
        å¥åº·æ£€æŸ¥

        Returns:
            æ˜¯å¦å¥åº·
        """
        try:
            # ç®€å•çš„æµ‹è¯•è¯·æ±‚
            test_request = LLMRequest(prompt="test", max_tokens=10)
            response = self.generate(test_request)
            return bool(response.content)
        except Exception:
            return False


class ProviderError(Exception):
    """æä¾›å•†é”™è¯¯"""
    pass
```

---

## ğŸ”§ æä¾›å•†å®ç°

### 1. é€šä¹‰åƒé—®æä¾›å•†

```python
"""
é€šä¹‰åƒé—® Qwen 3 æä¾›å•†
æ”¯æŒ Qwen 3 Plus / Max / Flash ç­‰æ¨¡å‹
"""

from typing import List, Dict, Any
import httpx
from ..base_LLM_provider import BaseLLMProvider, LLMRequest, LLMResponse, ProviderError


class QwenProvider(BaseLLMProvider):
    """
    é€šä¹‰åƒé—®æä¾›å•†

    API æ–‡æ¡£: https://help.aliyun.com/zh/model-studio/compatibility-of-openai-with-dashscope
    """

    # å¯ç”¨æ¨¡å‹åˆ—è¡¨
    MODELS = {
        "qwen-plus": {
            "name": "Qwen 3 Plus",
            "description": "ç»¼åˆæœ€ä½³æ¨¡å‹",
            "max_tokens": 8000,
            "context_length": 32000,
        },
        "qwen3-max": {
            "name": "Qwen 3 Max",
            "description": "æœ€å¼ºæ€§èƒ½æ¨¡å‹",
            "max_tokens": 8000,
            "context_length": 128000,
        },
        "qwen-flash": {
            "name": "Qwen Flash",
            "description": "è¶…å¿«å“åº”æ¨¡å‹",
            "max_tokens": 6000,
            "context_length": 32000,
        },
        "qwq-plus": {
            "name": "QwQ Plus",
            "description": "æ¨ç†èƒ½åŠ›æ¨¡å‹",
            "max_tokens": 32768,
            "context_length": 32768,
        },
    }

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
    ):
        """
        åˆå§‹åŒ–æä¾›å•†

        Args:
            api_key: API å¯†é’¥
            base_url: API åŸºç¡€ URL
        """
        super().__init__(api_key, base_url)
        self.http_client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            timeout=60.0,
        )

    def _get_model_name(self, model: str) -> str:
        """è·å–æ¨¡å‹å®é™…åç§°"""
        if model == "default":
            return "qwen-plus"
        if model in self.MODELS:
            return model
        raise ValueError(f"Unknown model: {model}")

    async def generate(self, request: LLMRequest) -> LLMResponse:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            request: LLM è¯·æ±‚

        Returns:
            LLM å“åº”
        """
        model = self._get_model_name(request.model)

        # æ„å»ºæ¶ˆæ¯
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})

        # è°ƒç”¨ API
        try:
            response = await self.http_client.post(
                f"{self.base_url}/chat/completions",
                json={
                    "model": model,
                    "messages": messages,
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                },
            )

            data = response.json()

            # è§£æå“åº”
            if "error" in data:
                raise ProviderError(data["error"]["message"])

            choice = data["choices"][0]
            content = choice["message"]["content"]

            return LLMResponse(
                content=content,
                model=model,
                tokens_used=data.get("usage", {}).get("total_tokens", 0),
                finish_reason=choice.get("finish_reason", "stop"),
            )

        except httpx.HTTPStatusError as e:
            raise ProviderError(f"HTTP é”™è¯¯: {e.response.status_code}")
        except Exception as e:
            raise ProviderError(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    async def generate_batch(
        self,
        requests: List[LLMRequest],
    ) -> List[LLMResponse]:
        """
        æ‰¹é‡ç”Ÿæˆæ–‡æœ¬

        Args:
            requests: LLM è¯·æ±‚åˆ—è¡¨

        Returns:
            LLM å“åº”åˆ—è¡¨
        """
        responses = []
        for request in requests:
            response = await self.generate(request)
            responses.append(response)
        return responses

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        return list(self.MODELS.keys())

    def get_model_info(self, model: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return self.MODELS.get(model, {})
```

### 2. Kimi æä¾›å•†

```python
"""
Kimi (æœˆä¹‹æš—é¢) æä¾›å•†
æ”¯æŒ Kimi 2.5
"""

from typing import List, Dict, Any
import httpx
from ..base_LLM_provider import BaseLLMProvider, LLMRequest, LLMResponse, ProviderError


class KimiProvider(BaseLLMProvider):
    """
    Kimi æä¾›å•†

    API æ–‡æ¡£: https://platform.moonshot.cn/docs
    """

    MODELS = {
        "kimi-2.5": {
            "name": "Kimi 2.5",
            "description": "æœ€æ–°ç‰ˆæœ¬",
            "max_tokens": 4000,
            "context_length": 128000,
        },
    }

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://api.moonshot.cn/v1",
    ):
        super().__init__(api_key, base_url)
        self.http_client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            timeout=60.0,
        )

    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬"""
        model = request.model if request.model != "default" else "kimi-2.5"

        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})

        try:
            response = await self.http_client.post(
                f"{self.base_url}/chat/completions",
                json={
                    "model": model,
                    "messages": messages,
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                },
            )

            data = response.json()
            choice = data["choices"][0]

            return LLMResponse(
                content=choice["message"]["content"],
                model=model,
                tokens_used=data.get("usage", {}).get("total_tokens", 0),
                finish_reason=choice.get("finish_reason", "stop"),
            )

        except Exception as e:
            raise ProviderError(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    async def generate_batch(
        self,
        requests: List[LLMRequest],
    ) -> List[LLMResponse]:
        responses = []
        for request in requests:
            responses.append(await self.generate(request))
        return responses

    def get_available_models(self) -> List[str]:
        return list(self.MODELS.keys())

    def get_model_info(self, model: str) -> Dict[str, Any]:
        return self.MODELS.get(model, {})
```

### 3. GLM-5 æä¾›å•†

```python
"""
æ™ºè°± GLM-5 æä¾›å•†
"""

from typing import List, Dict, Any
import httpx
from ..base_LLM_provider import BaseLLMProvider, LLMRequest, LLMResponse, ProviderError


class GLM5Provider(BaseLLMProvider):
    """æ™ºè°± GLM-5 æä¾›å•†"""

    MODELS = {
        "glm-5": {
            "name": "GLM-5",
            "description": "æ­£å¼ç‰ˆ",
            "max_tokens": 8000,
            "context_length": 128000,
        },
    }

    def __init__(
        self,
        api_key: str,
        base_url: str = "https://open.bigmodel.cn/api/paas/v4/",
    ):
        super().__init__(api_key, base_url)
        self.http_client = httpx.AsyncClient(
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            timeout=60.0,
        )

    async def generate(self, request: LLMRequest) -> LLMResponse:
        """ç”Ÿæˆæ–‡æœ¬"""
        model = request.model if request.model != "default" else "glm-5"

        try:
            response = await self.http_client.post(
                f"{self.base_url}chat/completions",
                json={
                    "model": model,
                    "messages": [
                        {"role": "user", "content": request.prompt},
                    ],
                    "max_tokens": request.max_tokens,
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                },
            )

            data = response.json()
            choice = data["choices"][0]

            return LLMResponse(
                content=choice["message"]["content"],
                model=model,
                tokens_used=data.get("usage", {}).get("total_tokens", 0),
            )

        except Exception as e:
            raise ProviderError(f"ç”Ÿæˆå¤±è´¥: {str(e)}")

    async def generate_batch(self, requests: List[LLMRequest]) -> List[LLMResponse]:
        return [await self.generate(req) for req in requests]

    def get_available_models(self) -> List[str]:
        return list(self.MODELS.keys())

    def get_model_info(self, model: str) -> Dict[str, Any]:
        return self.MODELS.get(model, {})
```

---

## ğŸ® LLM ç®¡ç†å™¨

### LLMManager

```python
"""
LLM ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ LLM æä¾›å•†ï¼Œæ”¯æŒè‡ªåŠ¨åˆ‡æ¢å’Œè´Ÿè½½å‡è¡¡
"""

from typing import Dict, Optional, List, Any
from enum import Enum

from .base_LLM_provider import (
    BaseLLMProvider,
    LLMRequest,
    LLMResponse,
    ProviderError,
)
from .providers.qwen import QwenProvider
from .providers.kimi import KimiProvider
from .providers.glm5 import GLM5Provider
from .providers.openai import OpenAIProvider


class ProviderType(Enum):
    """æä¾›å•†ç±»å‹"""
    QWEN = "qwen"
    KIMI = "kimi"
    GLM5 = "glm5"
    OPENAI = "openai"
    LOCAL = "local"


class LLMManager:
    """
    LLM ç®¡ç†å™¨

    åŠŸèƒ½:
    1. ç»Ÿä¸€æ¥å£è®¿é—®æ‰€æœ‰æä¾›å•†
    2. è‡ªåŠ¨åˆ‡æ¢å¤±è´¥æä¾›å•†
    3. è´Ÿè½½å‡è¡¡ï¼ˆå¯é€‰ï¼‰
    4. é…ç½®é©±åŠ¨
    """

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨

        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.providers: Dict[ProviderType, BaseLLMProvider] = {}
        self._default_provider: Optional[ProviderType] = None

        # åˆå§‹åŒ–æä¾›å•†
        self._init_providers()

    def _init_providers(self):
        """åˆå§‹åŒ–æ‰€æœ‰æä¾›å•†"""
        # é€šä¹‰åƒé—®
        if self.config.get("qwen", {}).get("api_key"):
            qwen_config = self.config["qwen"]
            self.providers[ProviderType.QWEN] = QwenProvider(
                api_key=qwen_config["api_key"],
                base_url=qwen_config.get("base_url", ""),
            )

        # Kimi
        if self.config.get("kimi", {}).get("api_key"):
            kimi_config = self.config["kimi"]
            self.providers[ProviderType.KIMI] = KimiProvider(
                api_key=kimi_config["api_key"],
                base_url=kimi_config.get("base_url", ""),
            )

        # GLM-5
        if self.config.get("glm5", {}).get("api_key"):
            glm5_config = self.config["glm5"]
            self.providers[ProviderType.GLM5] = GLM5Provider(
                api_key=glm5_config["api_key"],
                base_url=glm5_config.get("base_url", ""),
            )

        # OpenAI
        if self.config.get("openai", {}).get("api_key"):
            openai_config = self.config["openai"]
            self.providers[ProviderType.OPENAI] = OpenAIProvider(
                api_key=openai_config["api_key"],
                base_url=openai_config.get("base_url", ""),
            )

        # è®¾ç½®é»˜è®¤æä¾›å•†
        default_name = self.config.get("default_provider", "qwen")
        try:
            self._default_provider = ProviderType(default_name)
        except ValueError:
            if self.providers:
                self._default_provider = list(self.providers.keys())[0]

    async def generate(
        self,
        request: LLMRequest,
        provider: Optional[ProviderType] = None,
    ) -> LLMResponse:
        """
        ç”Ÿæˆæ–‡æœ¬

        Args:
            request: LLM è¯·æ±‚
            provider: æŒ‡å®šæä¾›å•†ï¼ˆå¯é€‰ï¼‰

        Returns:
            LLM å“åº”
        """
        # ç¡®å®šä½¿ç”¨çš„æä¾›å•†
        if provider and provider in self.providers:
            active_provider = self.providers[provider]
        else:
            provider = self._default_provider
            if not provider:
                raise ProviderError("æ²¡æœ‰å¯ç”¨çš„æä¾›å•†")
            active_provider = self.providers[provider]

        try:
            return await active_provider.generate(request)

        except ProviderError as e:
            # è‡ªåŠ¨åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†
            print(f"æä¾›å•† {provider.value} å¤±è´¥ï¼Œå°è¯•åˆ‡æ¢...")
            return await self._try_fallback(request, exclude=[provider])

    async def _try_fallback(
        self,
        request: LLMRequest,
        exclude: List[ProviderType],
    ) -> LLMResponse:
        """å°è¯•å¤‡ç”¨æä¾›å•†"""
        for provider_type, provider in self.providers.items():
            if provider_type not in exclude:
                try:
                    return await provider.generate(request)
                except ProviderError:
                    continue

        raise ProviderError("æ‰€æœ‰æä¾›å•†å‡å¤±è´¥")

    def get_provider(self, provider_type: ProviderType) -> BaseLLMProvider:
        """è·å–æŒ‡å®šæä¾›å•†"""
        if provider_type not in self.providers:
            raise ValueError(f"æä¾›å•† {provider_type} ä¸å¯ç”¨")
        return self.providers[provider_type]

    def get_available_providers(self) -> List[ProviderType]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        return list(self.providers.keys())

    def health_check(self) -> Dict[ProviderType, bool]:
        """å¥åº·æ£€æŸ¥æ‰€æœ‰æä¾›å•†"""
        results = {}
        for provider_type, provider in self.providers.items():
            results[provider_type] = provider.health_check()
        return results
```

---

## âš™ï¸ é…ç½®ç®¡ç†

### LLM é…ç½®æ–‡ä»¶

**æ–‡ä»¶**: `config/llm.yaml`

```yaml
# LLM é…ç½®

# é»˜è®¤æä¾›å•†
default_provider: qwen  # qwen | kimi | glm5 | openai | local

# é€šä¹‰åƒé—®
qwen:
  api_key: ${QWEN_API_KEY}  # ä»ç¯å¢ƒå˜é‡è¯»å–
  base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
  model: qwen-plus
  max_tokens: 2000
  temperature: 0.7

# Kimi
kimi:
  api_key: ${KIMI_API_KEY}
  base_url: https://api.moonshot.cn/v1
  model: kimi-2.5
  max_tokens: 2000
  temperature: 0.7

# GLM-5
glm5:
  api_key: ${GLM5_API_KEY}
  base_url: https://open.bigmodel.cn/api/paas/v4/
  model: glm-5
  max_tokens: 2000
  temperature: 0.7

# OpenAI (å¯é€‰)
openai:
  api_key: ${OPENAI_API_KEY}
  base_url: https://api.openai.com/v1
  model: gpt-4
  max_tokens: 2000
  temperature: 0.7
```

---

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### æ›´æ–° ScriptGenerator

```python
"""
æ›´æ–°åçš„æ–‡æ¡ˆç”Ÿæˆå™¨
ä½¿ç”¨ LLM Manager è€Œä¸æ˜¯ç›´æ¥ä¾èµ– OpenAI
"""

from .llm_manager import LLMManager, ProviderType
from .base_LLM_provider import LLMRequest, LLMResponse
from typing import Optional, Dict, Any
from dataclasses import dataclass


@dataclass
class ScriptConfig:
    """æ–‡æ¡ˆç”Ÿæˆé…ç½®"""
    style: str = "commentary"
    tone: str = "neutral"
    target_duration: float = 60.0
    provider: Optional[str] = None  # æŒ‡å®šæä¾›å•†


class ScriptGenerator:
    """
    AI æ–‡æ¡ˆç”Ÿæˆå™¨

    ç°åœ¨æ”¯æŒå¤šæä¾›å•†è‡ªåŠ¨åˆ‡æ¢
    """

    def __init__(
        self,
        llm_manager: Optional[LLMManager] = None,
        config: Optional[Dict[str, Any]] = None,
    ):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            llm_manager: LLM ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
            config: é…ç½®å­—å…¸
        """
        if llm_manager:
            self.llm_manager = llm_manager
        elif config:
            self.llm_manager = LLMManager(config)
        else:
            from .config import load_llm_config
            self.llm_manager = LLMManager(load_llm_config())

    async def generate(
        self,
        topic: str,
        config: Optional[ScriptConfig] = None,
    ) -> LLMResponse:
        """
        ç”Ÿæˆæ–‡æ¡ˆ

        Args:
            topic: ä¸»é¢˜
            config: é…ç½®

        Returns:
            LLM å“åº”
        """
        config = config or ScriptConfig()

        # æ„å»ºè¯·æ±‚
        request = LLMRequest(
            prompt=self._build_prompt(topic, config),
            system_prompt=self._get_system_prompt(config.style),
            model=config.get("model", "default"),
            max_tokens=int(config.target_duration * 3),  # çº¦3å­—/ç§’
            temperature=0.7,
        )

        # ç¡®å®šæä¾›å•†
        provider = None
        if config.provider:
            try:
                provider = ProviderType(config.provider)
            except ValueError:
                pass

        # ç”Ÿæˆ
        response = await self.llm_manager.generate(request, provider=provider)

        return response

    def _build_prompt(self, topic: str, config: ScriptConfig) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯"""
        return f"è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆè§†é¢‘æ–‡æ¡ˆï¼š\n\n{topic}"

    def _get_system_prompt(self, style: str) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        prompts = {
            "commentary": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†é¢‘è§£è¯´æ–‡æ¡ˆæ’°å†™è€…ã€‚",
            "monologue": "ä½ æ˜¯ä¸€ä½æ“…é•¿å†™ç¬¬ä¸€äººç§°ç‹¬ç™½çš„æ–‡æ¡ˆä½œè€…ã€‚",
            "viral": "ä½ æ˜¯ä¸€ä½çˆ†æ¬¾çŸ­è§†é¢‘æ–‡æ¡ˆé«˜æ‰‹ã€‚",
        }
        return prompts.get(style, "ä½ æ˜¯ä¸€ä½æ–‡æ¡ˆæ’°å†™è€…ã€‚")
```

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’

### å•å…ƒæµ‹è¯•

```python
"""
LLM æä¾›å•†å•å…ƒæµ‹è¯•
"""

import pytest
from app.services.ai.providers.qwen import QwenProvider
from app.services.ai.base_LLM_provider import LLMRequest


@pytest.mark.asyncio
async def test_qwen_provider():
    """æµ‹è¯•é€šä¹‰åƒé—®æä¾›å•†"""
    provider = QwenProvider(api_key="test-key")
    request = LLMRequest(prompt="æµ‹è¯•", max_tokens=10)

    # ä½¿ç”¨ Mock
    # response = await provider.generate(request)
    # assert response.content
```

### é›†æˆæµ‹è¯•

```python
"""
LLM ç®¡ç†å™¨é›†æˆæµ‹è¯•
"""

from app.services.ai.llm_manager import LLMManager


async def test_llm_manager():
    """æµ‹è¯• LLM ç®¡ç†å™¨"""
    config = {
        "default_provider": "qwen",
        "qwen": {"api_key": "test-key"},
    }

    manager = LLMManager(config)

    # æµ‹è¯•ç”Ÿæˆ
    # response = await manager.generate(test_request)
    # assert response.content
```

---

## ğŸ“Š è¿ç§»è®¡åˆ’

### ç«‹å³æ‰§è¡Œ

1. [ ] åˆ›å»º `app/services/ai/base_LLM_provider.py`
2. [ ] å®ç°é€šä¹‰åƒé—®æä¾›å•†
3. [ ] å®ç° Kimi æä¾›å•†
4. [ ] å®ç° GLM-5 æä¾›å•†
5. [ ] åˆ›å»º LLM ç®¡ç†å™¨
6. [ ] æ›´æ–° ScriptGenerator ä½¿ç”¨æ–°æ¶æ„

### åç»­å·¥ä½œ

1. [ ] æ·»åŠ ç™¾åº¦æ–‡å¿ƒæä¾›å•†
2. [ ] æ·»åŠ æœ¬åœ°æ¨¡å‹æ”¯æŒï¼ˆOllama ç­‰ï¼‰
3. [ ] å®ç°è´Ÿè½½å‡è¡¡
4. [ ] æ·»åŠ ç¼“å­˜å±‚
5. [ ] å®Œå–„å•å…ƒæµ‹è¯•

---

## ğŸ‰ æ€»ç»“

### å…³é”®æ”¹è¿›

1. **æŠ½è±¡æ¥å£**: ç»Ÿä¸€çš„ LLMProvider æ¥å£
2. **å¤šæä¾›å•†**: æ”¯æŒå›½äº§ä¸»æµ LLM
3. **è‡ªåŠ¨åˆ‡æ¢**: å¤±è´¥è‡ªåŠ¨åˆ‡æ¢å¤‡ç”¨æä¾›å•†
4. **é…ç½®é©±åŠ¨**: YAML é…ç½®ç®¡ç†
5. **æ˜“äºæµ‹è¯•**: æ”¯æŒ Mock å’Œ Stub

### ä¸‹ä¸€æ­¥

- [ ] å®æ–½ç¼–ç 
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•
- [ ] æ›´æ–°ç°æœ‰ä»£ç é›†æˆæ–°æ¶æ„

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ
**å®æ–½çŠ¶æ€**: â³ å¾…æ‰§è¡Œ
