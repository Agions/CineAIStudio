#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
CineFlow AI LLM ç¼“å­˜å’Œé‡è¯•æœºåˆ¶
æé«˜ LLM API è°ƒç”¨æ€§èƒ½å’Œå¯é æ€§
"""

import hashlib
import json
import time
from typing import Optional, Dict, Any, Callable
from functools import wraps
from datetime import datetime, timedelta
from pathlib import Path


class LLMMemoryCache:
    """LLM å“åº”å†…å­˜ç¼“å­˜"""

    def __init__(self, max_size: int = 100, ttl: int = 3600):
        """
        åˆå§‹åŒ–ç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ (ç§’)
        """
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.max_size = max_size
        self.ttl = ttl
        self.access_order: list = []

    def _generate_key(
        self,
        messages: list,
        model: str,
        temperature: Optional[float] = None
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åˆ›å»ºåŒ…å«æ‰€æœ‰å‚æ•°çš„å­—ç¬¦ä¸²
        data = {
            "messages": messages,
            "model": model,
            "temperature": temperature
        }
        data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)

        # ä½¿ç”¨ MD5 å“ˆå¸Œç”Ÿæˆé”®
        return hashlib.md5(data_str.encode()).hexdigest()

    def get(
        self,
        messages: list,
        model: str,
        temperature: Optional[float] = None
    ) -> Optional[str]:
        """è·å–ç¼“å­˜å“åº”"""
        key = self._generate_key(messages, model, temperature)

        if key in self.cache:
            entry = self.cache[key]

            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - entry["timestamp"] < self.ttl:
                # æ›´æ–°è®¿é—®é¡ºåº
                if key in self.access_order:
                    self.access_order.remove(key)
                self.access_order.append(key)

                return entry["response"]
            else:
                # å·²è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[key]
                if key in self.access_order:
                    self.access_order.remove(key)

        return None

    def set(
        self,
        messages: list,
        model: str,
        response: str,
        temperature: Optional[float] = None
    ) -> None:
        """è®¾ç½®ç¼“å­˜"""
        key = self._generate_key(messages, model, temperature)

        # å¦‚æœè¾¾åˆ°æœ€å¤§å®¹é‡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
        if len(self.cache) >= self.max_size:
            if self.access_order:
                oldest_key = self.access_order.pop(0)
                if oldest_key in self.cache:
                    del self.cache[oldest_key]

        # æ·»åŠ æ–°æ¡ç›®
        self.cache[key] = {
            "response": response,
            "timestamp": time.time()
        }
        self.access_order.append(key)

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_order.clear()

    def get_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "size": len(self.cache),
            "max_size": self.max_size,
            "ttl": self.ttl
        }


class LLMRetryPolicy:
    """LLM è¯·æ±‚é‡è¯•ç­–ç•¥"""

    def __init__(
        self,
        max_retries: int = 3,
        base_delay: float = 1.0,
        max_delay: float = 60.0,
        backoff_factor: float = 2.0
    ):
        """
        åˆå§‹åŒ–é‡è¯•ç­–ç•¥

        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            base_delay: åŸºç¡€å»¶è¿Ÿæ—¶é—´ (ç§’)
            max_delay: æœ€å¤§å»¶è¿Ÿæ—¶é—´ (ç§’)
            backoff_factor: é€€é¿å› å­ (æŒ‡æ•°é€€é¿)
        """
        self.max_retries = max_retries
        self.base_delay = base_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor

    def get_delay(self, retry_count: int) -> float:
        """
        è®¡ç®—ä¸‹ä¸€æ¬¡é‡è¯•çš„å»¶è¿Ÿæ—¶é—´

        Args:
            retry_count: å·²é‡è¯•æ¬¡æ•°

        Returns:
            å»¶è¿Ÿæ—¶é—´ (ç§’)
        """
        delay = self.base_delay * (self.backoff_factor ** retry_count)
        return min(delay, self.max_delay)


def with_retry(
    policy: Optional[LLMRetryPolicy] = None,
    exceptions: tuple = (Exception,),
    on_retry: Optional[Callable[[int, Exception], None]] = None
):
    """
    è£…é¥°å™¨: ä¸ºå‡½æ•°æ·»åŠ é‡è¯•åŠŸèƒ½

    Args:
        policy: é‡è¯•ç­–ç•¥
        exceptions: éœ€è¦é‡è¯•çš„å¼‚å¸¸ç±»å‹
        on_retry: é‡è¯•å›è°ƒå‡½æ•°
    """
    if policy is None:
        policy = LLMRetryPolicy()

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            last_exception = None

            for attempt in range(policy.max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e

                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt >= policy.max_retries:
                        break

                    # è®¡ç®—å»¶è¿Ÿ
                    delay = policy.get_delay(attempt)

                    # è°ƒç”¨é‡è¯•å›è°ƒ
                    if on_retry:
                        on_retry(attempt + 1, e)

                    # ç­‰å¾…åé‡è¯•
                    if delay > 0:
                        time.sleep(delay)

            raise last_exception

        return wrapper
    return decorator


class LLMPerformanceMonitor:
    """LLM æ€§èƒ½ç›‘æ§"""

    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }

    def record_request(
        self,
        success: bool,
        tokens: Optional[int] = None,
        time_taken: Optional[float] = None
    ) -> None:
        """è®°å½•è¯·æ±‚"""
        self.metrics["total_requests"] += 1

        if success:
            self.metrics["successful_requests"] += 1
            if tokens:
                self.metrics["total_tokens"] += tokens
            if time_taken:
                self.metrics["total_time"] += time_taken
        else:
            self.metrics["failed_requests"] += 1

    def record_cache_hit(self) -> None:
        """è®°å½•ç¼“å­˜å‘½ä¸­"""
        self.metrics["cache_hits"] += 1

    def record_cache_miss(self) -> None:
        """è®°å½•ç¼“å­˜æœªå‘½ä¸­"""
        self.metrics["cache_misses"] += 1

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.metrics.copy()

        # è®¡ç®—å‘½ä¸­ç‡
        total_cache_access = self.metrics["cache_hits"] + self.metrics["cache_misses"]
        if total_cache_access > 0:
            stats["cache_hit_rate"] = self.metrics["cache_hits"] / total_cache_access

        # è®¡ç®—æˆåŠŸç‡
        if self.metrics["total_requests"] > 0:
            stats["success_rate"] = self.metrics["successful_requests"] / self.metrics["total_requests"]

        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        if self.metrics["successful_requests"] > 0:
            stats["avg_response_time"] = self.metrics["total_time"] / self.metrics["successful_requests"]
        else:
            stats["avg_response_time"] = 0.0

        # è®¡ç®—æ€»èŠ±è´¹ (æŒ‰ 0.001 å…ƒ / token ä¼°ç®—)
        stats["estimated_cost"] = self.metrics["total_tokens"] * 0.001

        return stats

    def print_stats(self) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_stats()

        print("\nğŸ“Š LLM æ€§èƒ½ç»Ÿè®¡")
        print("=" * 50)
        print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"æˆåŠŸè¯·æ±‚: {stats['successful_requests']}")
        print(f"å¤±è´¥è¯·æ±‚: {stats['failed_requests']}")

        if "success_rate" in stats:
            print(f"æˆåŠŸç‡: {stats['success_rate']:.1%}")

        print(f"\nç¼“å­˜å‘½ä¸­: {stats['cache_hits']}")
        print(f"ç¼“å­˜æœªå‘½ä¸­: {stats['cache_misses']}")

        if "cache_hit_rate" in stats:
            print(f"ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1%}")

        print(f"\næ€» Token æ•°: {stats['total_tokens']}")
        print(f"ä¼°è®¡æˆæœ¬: Â¥{stats['estimated_cost']:.2f}")

        if stats['avg_response_time'] > 0:
            print(f"å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f} ç§’")

        print("=" * 50)

    def reset(self) -> None:
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens": 0,
            "total_time": 0.0,
            "cache_hits": 0,
            "cache_misses": 0
        }


# å…¨å±€ç¼“å­˜å’Œç›‘æ§å®ä¾‹
_global_cache = LLMMemoryCache()
_global_monitor = LLMPerformanceMonitor()


def get_global_cache() -> LLMMemoryCache:
    """è·å–å…¨å±€ç¼“å­˜å®ä¾‹"""
    return _global_cache


def get_global_monitor() -> LLMPerformanceMonitor:
    """è·å–å…¨å±€ç›‘æ§å®ä¾‹"""
    return _global_monitor
