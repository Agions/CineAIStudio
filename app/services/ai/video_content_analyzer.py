"""
è§†é¢‘å†…å®¹åˆ†æå™¨ (Video Content Analyzer)

æ·±åº¦åˆ†æè§†é¢‘ç”»é¢å†…å®¹ï¼Œä¸º AI è§£è¯´ã€æ··å‰ªã€ç‹¬ç™½æä¾›ç´ æç†è§£ã€‚

åŠŸèƒ½:
- æå–å…³é”®å¸§
- åˆ†æç”»é¢å†…å®¹ï¼ˆæ”¯æŒ OpenAI Vision / æœ¬åœ°æ¨¡å‹ï¼‰
- è¯†åˆ«åœºæ™¯ç±»å‹
- æå–æ–‡å­—ï¼ˆOCRï¼‰
- æ£€æµ‹äººè„¸å’Œè¡¨æƒ…

ä½¿ç”¨ç¤ºä¾‹:
    from app.services.ai import VideoContentAnalyzer
    
    analyzer = VideoContentAnalyzer(vision_api_key="your-key")
    
    # åˆ†æè§†é¢‘
    result = analyzer.analyze("video.mp4")
    
    print(f"è§†é¢‘ä¸»é¢˜: {result.summary}")
    for frame in result.keyframes:
        print(f"  å¸§ {frame.timestamp}s: {frame.description}")
"""

import os
import subprocess
import base64
import json
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor


class ContentType(Enum):
    """å†…å®¹ç±»å‹"""
    PERSON = "person"              # äººç‰©
    LANDSCAPE = "landscape"        # é£æ™¯
    INDOOR = "indoor"              # å®¤å†…
    OUTDOOR = "outdoor"            # å®¤å¤–
    TEXT = "text"                  # æ–‡å­—/å­—å¹•
    PRODUCT = "product"            # äº§å“
    ANIMAL = "animal"              # åŠ¨ç‰©
    FOOD = "food"                  # ç¾é£Ÿ
    VEHICLE = "vehicle"            # è½¦è¾†
    BUILDING = "building"          # å»ºç­‘
    ACTION = "action"              # åŠ¨ä½œåœºæ™¯
    UNKNOWN = "unknown"


class Emotion(Enum):
    """æƒ…æ„Ÿç±»å‹"""
    NEUTRAL = "neutral"
    HAPPY = "happy"
    SAD = "sad"
    EXCITED = "excited"
    CALM = "calm"
    TENSE = "tense"
    ROMANTIC = "romantic"


@dataclass
class KeyframeInfo:
    """å…³é”®å¸§ä¿¡æ¯"""
    index: int                     # å¸§åºå·
    timestamp: float               # æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
    image_path: str = ""           # å›¾ç‰‡è·¯å¾„
    
    # ç”»é¢åˆ†æ
    description: str = ""          # ç”»é¢æè¿°
    content_type: ContentType = ContentType.UNKNOWN
    objects: List[str] = field(default_factory=list)  # æ£€æµ‹åˆ°çš„ç‰©ä½“
    text_content: str = ""         # OCR è¯†åˆ«çš„æ–‡å­—
    
    # æƒ…æ„Ÿå’Œæ°›å›´
    emotion: Emotion = Emotion.NEUTRAL
    color_tone: str = ""           # è‰²è°ƒï¼ˆæš–/å†·/ä¸­æ€§ï¼‰
    brightness: float = 0.5        # äº®åº¦ (0-1)
    
    # é€‚ç”¨æ€§
    suitability: Dict[str, float] = field(default_factory=dict)  # ä¸åŒç”¨é€”çš„é€‚ç”¨æ€§åˆ†æ•°


@dataclass
class VideoAnalysisResult:
    """è§†é¢‘åˆ†æç»“æœ"""
    video_path: str
    duration: float                # è§†é¢‘æ—¶é•¿
    resolution: Tuple[int, int]    # åˆ†è¾¨ç‡
    fps: float                     # å¸§ç‡
    
    # å…³é”®å¸§
    keyframes: List[KeyframeInfo] = field(default_factory=list)
    
    # æ•´ä½“åˆ†æ
    summary: str = ""              # è§†é¢‘å†…å®¹æ‘˜è¦
    main_topics: List[str] = field(default_factory=list)  # ä¸»è¦ä¸»é¢˜
    main_emotion: Emotion = Emotion.NEUTRAL  # ä¸»è¦æƒ…æ„Ÿ
    suggested_style: str = ""      # å»ºè®®çš„è§£è¯´/ç‹¬ç™½é£æ ¼
    
    # å…³é”®è¯
    keywords: List[str] = field(default_factory=list)
    
    # è„šæœ¬å»ºè®®
    script_suggestion: str = ""    # æ–‡æ¡ˆå»ºè®®


@dataclass
class AnalyzerConfig:
    """åˆ†æå™¨é…ç½®"""
    # å…³é”®å¸§æå–
    keyframe_interval: float = 2.0  # æ¯éš”å¤šå°‘ç§’æå–ä¸€å¸§
    max_keyframes: int = 30         # æœ€å¤§å…³é”®å¸§æ•°
    keyframe_quality: int = 95      # JPEG è´¨é‡
    
    # åˆ†æé€‰é¡¹
    use_vision_api: bool = True     # æ˜¯å¦ä½¿ç”¨è§†è§‰APIåˆ†æ
    use_ocr: bool = True            # æ˜¯å¦OCRè¯†åˆ«æ–‡å­—
    parallel_analysis: bool = True  # æ˜¯å¦å¹¶è¡Œåˆ†æ
    
    # API é…ç½®
    vision_provider: str = "openai"  # openai / local / qwen


class VideoContentAnalyzer:
    """
    è§†é¢‘å†…å®¹åˆ†æå™¨
    
    æ·±åº¦åˆ†æè§†é¢‘ç”»é¢å†…å®¹ï¼Œç†è§£è§†é¢‘ä¸»é¢˜å’Œæƒ…æ„Ÿ
    
    ä½¿ç”¨ç¤ºä¾‹:
        analyzer = VideoContentAnalyzer(
            vision_api_key="sk-xxx",
            config=AnalyzerConfig(keyframe_interval=3.0)
        )
        
        result = analyzer.analyze("video.mp4")
        
        print(f"è§†é¢‘æ‘˜è¦: {result.summary}")
        print(f"ä¸»è¦ä¸»é¢˜: {result.main_topics}")
        print(f"å»ºè®®é£æ ¼: {result.suggested_style}")
    """
    
    def __init__(
        self,
        vision_api_key: Optional[str] = None,
        config: Optional[AnalyzerConfig] = None,
    ):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            vision_api_key: è§†è§‰APIå¯†é’¥ï¼ˆOpenAI/é€šä¹‰åƒé—®ï¼‰
            config: åˆ†æé…ç½®
        """
        self.api_key = vision_api_key or os.getenv("OPENAI_API_KEY")
        self.config = config or AnalyzerConfig()
        
        # ä¸´æ—¶ç›®å½•
        self._temp_dir = Path("./temp/keyframes")
        self._temp_dir.mkdir(parents=True, exist_ok=True)
    
    def analyze(self, video_path: str) -> VideoAnalysisResult:
        """
        åˆ†æè§†é¢‘å†…å®¹
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            è§†é¢‘åˆ†æç»“æœ
        """
        video_path = Path(video_path)
        if not video_path.exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        duration, width, height, fps = self._get_video_info(str(video_path))
        
        result = VideoAnalysisResult(
            video_path=str(video_path),
            duration=duration,
            resolution=(width, height),
            fps=fps,
        )
        
        # æå–å…³é”®å¸§
        keyframes = self._extract_keyframes(str(video_path), duration)
        
        # åˆ†æå…³é”®å¸§
        if self.config.use_vision_api and self.api_key:
            if self.config.parallel_analysis:
                self._analyze_keyframes_parallel(keyframes)
            else:
                self._analyze_keyframes_sequential(keyframes)
        else:
            # ä½¿ç”¨æœ¬åœ°åˆ†æï¼ˆåŸºäº FFmpegï¼‰
            self._analyze_keyframes_local(keyframes)
        
        result.keyframes = keyframes
        
        # ç”Ÿæˆæ•´ä½“åˆ†æ
        self._generate_summary(result)
        
        return result
    
    def _get_video_info(self, video_path: str) -> Tuple[float, int, int, float]:
        """è·å–è§†é¢‘ä¿¡æ¯"""
        cmd = [
            'ffprobe', '-v', 'quiet',
            '-show_entries', 'format=duration:stream=width,height,r_frame_rate',
            '-of', 'json',
            video_path
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                data = json.loads(result.stdout)
                
                duration = float(data.get('format', {}).get('duration', 0))
                
                stream = data.get('streams', [{}])[0]
                width = int(stream.get('width', 1920))
                height = int(stream.get('height', 1080))
                
                # è§£æå¸§ç‡
                fps_str = stream.get('r_frame_rate', '30/1')
                if '/' in fps_str:
                    num, den = fps_str.split('/')
                    fps = float(num) / float(den) if float(den) > 0 else 30.0
                else:
                    fps = float(fps_str)
                
                return duration, width, height, fps
        except Exception as e:
            print(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        
        return 0.0, 1920, 1080, 30.0
    
    def _extract_keyframes(
        self,
        video_path: str,
        duration: float,
    ) -> List[KeyframeInfo]:
        """æå–å…³é”®å¸§"""
        keyframes = []
        
        # è®¡ç®—æå–é—´éš”
        interval = self.config.keyframe_interval
        num_frames = min(
            int(duration / interval) + 1,
            self.config.max_keyframes
        )
        
        # è°ƒæ•´é—´éš”ä»¥å‡åŒ€åˆ†å¸ƒ
        if num_frames > 1:
            interval = duration / (num_frames - 1)
        
        video_name = Path(video_path).stem
        output_dir = self._temp_dir / video_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        for i in range(num_frames):
            timestamp = i * interval
            if timestamp > duration:
                break
            
            output_path = output_dir / f"frame_{i:04d}.jpg"
            
            # ä½¿ç”¨ FFmpeg æå–å¸§
            cmd = [
                'ffmpeg', '-y',
                '-ss', str(timestamp),
                '-i', video_path,
                '-vframes', '1',
                '-q:v', str(100 - self.config.keyframe_quality),
                str(output_path)
            ]
            
            result = subprocess.run(cmd, capture_output=True)
            
            if result.returncode == 0 and output_path.exists():
                keyframe = KeyframeInfo(
                    index=i,
                    timestamp=timestamp,
                    image_path=str(output_path),
                )
                keyframes.append(keyframe)
        
        return keyframes
    
    def _analyze_keyframes_parallel(self, keyframes: List[KeyframeInfo]) -> None:
        """å¹¶è¡Œåˆ†æå…³é”®å¸§"""
        with ThreadPoolExecutor(max_workers=4) as executor:
            list(executor.map(self._analyze_single_keyframe, keyframes))
    
    def _analyze_keyframes_sequential(self, keyframes: List[KeyframeInfo]) -> None:
        """é¡ºåºåˆ†æå…³é”®å¸§"""
        for keyframe in keyframes:
            self._analyze_single_keyframe(keyframe)
    
    def _analyze_single_keyframe(self, keyframe: KeyframeInfo) -> None:
        """åˆ†æå•ä¸ªå…³é”®å¸§"""
        if not keyframe.image_path or not Path(keyframe.image_path).exists():
            return
        
        # è¯»å–å›¾ç‰‡å¹¶è½¬ä¸º base64
        with open(keyframe.image_path, 'rb') as f:
            image_data = base64.b64encode(f.read()).decode('utf-8')
        
        # è°ƒç”¨è§†è§‰APIåˆ†æ
        try:
            if self.config.vision_provider == "openai":
                analysis = self._analyze_with_openai(image_data)
            else:
                analysis = self._analyze_with_local(keyframe.image_path)
            
            # è§£æç»“æœ
            keyframe.description = analysis.get("description", "")
            keyframe.objects = analysis.get("objects", [])
            keyframe.text_content = analysis.get("text", "")
            keyframe.content_type = self._parse_content_type(analysis.get("content_type", ""))
            keyframe.emotion = self._parse_emotion(analysis.get("emotion", ""))
            keyframe.color_tone = analysis.get("color_tone", "neutral")
            
        except Exception as e:
            print(f"åˆ†æå¸§ {keyframe.index} å¤±è´¥: {e}")
    
    def _analyze_with_openai(self, image_base64: str) -> Dict[str, Any]:
        """ä½¿ç”¨ OpenAI Vision API åˆ†æ"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            
            prompt = """åˆ†æè¿™å¼ è§†é¢‘æˆªå›¾ï¼Œè¿”å›JSONæ ¼å¼ï¼š
{
    "description": "è¯¦ç»†æè¿°ç”»é¢å†…å®¹ï¼ˆ50-100å­—ï¼‰",
    "content_type": "person/landscape/indoor/outdoor/text/product/animal/food/action",
    "objects": ["æ£€æµ‹åˆ°çš„ä¸»è¦ç‰©ä½“åˆ—è¡¨"],
    "text": "ç”»é¢ä¸­å‡ºç°çš„æ–‡å­—ï¼ˆå¦‚æœæœ‰ï¼‰",
    "emotion": "neutral/happy/sad/excited/calm/tense/romantic",
    "color_tone": "warm/cold/neutral",
    "suitable_for": {
        "commentary": 0-100,
        "monologue": 0-100,
        "mashup": 0-100
    }
}
åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚"""
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}",
                                    "detail": "low"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=500,
            )
            
            content = response.choices[0].message.content
            
            # å°è¯•è§£æ JSON
            try:
                # ç§»é™¤å¯èƒ½çš„ markdown ä»£ç å—æ ‡è®°
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0]
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0]
                
                return json.loads(content.strip())
            except json.JSONDecodeError:
                return {"description": content}
            
        except Exception as e:
            print(f"OpenAI Vision åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _analyze_with_local(self, image_path: str) -> Dict[str, Any]:
        """ä½¿ç”¨æœ¬åœ°æ–¹æ³•åˆ†æï¼ˆFFmpegä¿¡å·ç»Ÿè®¡ï¼‰"""
        # ä½¿ç”¨ FFmpeg è·å–å›¾åƒç»Ÿè®¡ä¿¡æ¯
        cmd = [
            'ffprobe', '-v', 'quiet',
            '-show_entries', 'frame=pkt_pts_time',
            '-show_entries', 'frame_tags=lavfi.signalstats.YAVG,lavfi.signalstats.SATAVG',
            '-of', 'json',
            image_path
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True)
            # åŸºäºäº®åº¦å’Œé¥±å’Œåº¦åšç®€å•åˆ¤æ–­
            # è¿™æ˜¯é™çº§æ–¹æ¡ˆï¼Œå‡†ç¡®åº¦æœ‰é™
            return {
                "description": "ç”»é¢å†…å®¹ï¼ˆéœ€è¦è§†è§‰APIè·å–è¯¦ç»†æè¿°ï¼‰",
                "content_type": "unknown",
                "objects": [],
                "emotion": "neutral",
            }
        except Exception:
            return {}
    
    def _analyze_keyframes_local(self, keyframes: List[KeyframeInfo]) -> None:
        """ä½¿ç”¨æœ¬åœ°æ–¹æ³•åˆ†æï¼ˆä¸ä½¿ç”¨APIï¼‰"""
        for keyframe in keyframes:
            if not keyframe.image_path:
                continue
            
            # ä½¿ç”¨ FFmpeg è·å–äº®åº¦ä¿¡æ¯
            brightness = self._get_image_brightness(keyframe.image_path)
            keyframe.brightness = brightness
            
            # åŸºäºäº®åº¦ç®€å•åˆ¤æ–­
            if brightness < 0.3:
                keyframe.color_tone = "dark"
                keyframe.emotion = Emotion.TENSE
            elif brightness > 0.7:
                keyframe.color_tone = "bright"
                keyframe.emotion = Emotion.HAPPY
            else:
                keyframe.color_tone = "neutral"
                keyframe.emotion = Emotion.NEUTRAL
            
            keyframe.description = f"ç”»é¢äº®åº¦ {brightness:.1%}"
    
    def _get_image_brightness(self, image_path: str) -> float:
        """è·å–å›¾åƒäº®åº¦"""
        cmd = [
            'ffmpeg', '-i', image_path,
            '-vf', 'signalstats,metadata=print:file=-',
            '-f', 'null', '-'
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            
            # è§£æ YAVG (äº®åº¦å¹³å‡å€¼)
            import re
            match = re.search(r'YAVG=(\d+\.?\d*)', result.stderr)
            if match:
                return float(match.group(1)) / 255.0
        except Exception:
            pass
        
        return 0.5
    
    def _parse_content_type(self, type_str: str) -> ContentType:
        """è§£æå†…å®¹ç±»å‹"""
        type_map = {
            "person": ContentType.PERSON,
            "landscape": ContentType.LANDSCAPE,
            "indoor": ContentType.INDOOR,
            "outdoor": ContentType.OUTDOOR,
            "text": ContentType.TEXT,
            "product": ContentType.PRODUCT,
            "animal": ContentType.ANIMAL,
            "food": ContentType.FOOD,
            "vehicle": ContentType.VEHICLE,
            "building": ContentType.BUILDING,
            "action": ContentType.ACTION,
        }
        return type_map.get(type_str.lower(), ContentType.UNKNOWN)
    
    def _parse_emotion(self, emotion_str: str) -> Emotion:
        """è§£ææƒ…æ„Ÿç±»å‹"""
        emotion_map = {
            "neutral": Emotion.NEUTRAL,
            "happy": Emotion.HAPPY,
            "sad": Emotion.SAD,
            "excited": Emotion.EXCITED,
            "calm": Emotion.CALM,
            "tense": Emotion.TENSE,
            "romantic": Emotion.ROMANTIC,
        }
        return emotion_map.get(emotion_str.lower(), Emotion.NEUTRAL)
    
    def _generate_summary(self, result: VideoAnalysisResult) -> None:
        """ç”Ÿæˆæ•´ä½“åˆ†ææ‘˜è¦"""
        if not result.keyframes:
            return
        
        # ç»Ÿè®¡å†…å®¹ç±»å‹
        type_counts = {}
        for kf in result.keyframes:
            t = kf.content_type.value
            type_counts[t] = type_counts.get(t, 0) + 1
        
        # æ‰¾å‡ºä¸»è¦ç±»å‹
        if type_counts:
            main_type = max(type_counts, key=type_counts.get)
            result.main_topics = [main_type]
        
        # ç»Ÿè®¡æƒ…æ„Ÿ
        emotion_counts = {}
        for kf in result.keyframes:
            e = kf.emotion.value
            emotion_counts[e] = emotion_counts.get(e, 0) + 1
        
        if emotion_counts:
            main_emotion_str = max(emotion_counts, key=emotion_counts.get)
            result.main_emotion = self._parse_emotion(main_emotion_str)
        
        # æ”¶é›†æè¿°
        descriptions = [kf.description for kf in result.keyframes if kf.description]
        
        # æ”¶é›†å…³é”®è¯
        all_objects = []
        for kf in result.keyframes:
            all_objects.extend(kf.objects)
        result.keywords = list(set(all_objects))[:20]
        
        # ç”Ÿæˆæ‘˜è¦
        if descriptions:
            result.summary = " ".join(descriptions[:5])
        else:
            result.summary = f"è§†é¢‘æ—¶é•¿ {result.duration:.1f}ç§’ï¼ŒåŒ…å« {len(result.keyframes)} ä¸ªå…³é”®åœºæ™¯"
        
        # å»ºè®®é£æ ¼
        if result.main_emotion == Emotion.SAD:
            result.suggested_style = "melancholic"
        elif result.main_emotion == Emotion.HAPPY:
            result.suggested_style = "cheerful"
        elif result.main_emotion == Emotion.EXCITED:
            result.suggested_style = "energetic"
        elif result.main_emotion == Emotion.CALM:
            result.suggested_style = "peaceful"
        else:
            result.suggested_style = "neutral"
        
        # ç”Ÿæˆè„šæœ¬å»ºè®®
        result.script_suggestion = self._generate_script_suggestion(result)
    
    def _generate_script_suggestion(self, result: VideoAnalysisResult) -> str:
        """ç”Ÿæˆè„šæœ¬å»ºè®®"""
        if not self.api_key:
            return ""
        
        # æ”¶é›†å…³é”®å¸§æè¿°
        descriptions = [
            f"[{kf.timestamp:.1f}s] {kf.description}"
            for kf in result.keyframes[:10]
            if kf.description
        ]
        
        if not descriptions:
            return ""
        
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=self.api_key)
            
            prompt = f"""åŸºäºä»¥ä¸‹è§†é¢‘å†…å®¹åˆ†æï¼Œç”Ÿæˆä¸€æ®µé€‚åˆè¿™ä¸ªè§†é¢‘çš„è§£è¯´/ç‹¬ç™½æ–‡æ¡ˆå»ºè®®ã€‚

è§†é¢‘ä¿¡æ¯:
- æ—¶é•¿: {result.duration:.1f}ç§’
- ä¸»è¦ä¸»é¢˜: {', '.join(result.main_topics)}
- æƒ…æ„ŸåŸºè°ƒ: {result.main_emotion.value}
- å…³é”®è¯: {', '.join(result.keywords[:10])}

ç”»é¢æè¿°:
{chr(10).join(descriptions)}

è¯·ç”Ÿæˆ:
1. ä¸€æ®µçº¦100å­—çš„è§£è¯´æ–‡æ¡ˆï¼ˆå®¢è§‚ã€ä¿¡æ¯åŒ–ï¼‰
2. ä¸€æ®µçº¦100å­—çš„ç‹¬ç™½æ–‡æ¡ˆï¼ˆç¬¬ä¸€äººç§°ã€æƒ…æ„ŸåŒ–ï¼‰

æ ¼å¼ï¼š
ã€è§£è¯´æ–‡æ¡ˆã€‘
...

ã€ç‹¬ç™½æ–‡æ¡ˆã€‘
..."""
            
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"ç”Ÿæˆè„šæœ¬å»ºè®®å¤±è´¥: {e}")
            return ""
    
    def cleanup(self) -> None:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        import shutil
        if self._temp_dir.exists():
            shutil.rmtree(self._temp_dir, ignore_errors=True)


def analyze_video(video_path: str, api_key: Optional[str] = None) -> VideoAnalysisResult:
    """
    åˆ†æè§†é¢‘å†…å®¹çš„ä¾¿æ·å‡½æ•°
    
    Args:
        video_path: è§†é¢‘è·¯å¾„
        api_key: è§†è§‰APIå¯†é’¥
        
    Returns:
        åˆ†æç»“æœ
    """
    analyzer = VideoContentAnalyzer(vision_api_key=api_key)
    return analyzer.analyze(video_path)


def demo_analyze():
    """æ¼”ç¤ºè§†é¢‘åˆ†æ"""
    print("=" * 50)
    print("è§†é¢‘å†…å®¹åˆ†æå™¨æ¼”ç¤º")
    print("=" * 50)
    
    analyzer = VideoContentAnalyzer()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•è§†é¢‘
    test_videos = list(Path("test_assets").glob("*.mp4"))
    
    if not test_videos:
        print("\nâš ï¸ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•è§†é¢‘")
        print("è¯·å°† .mp4 æ–‡ä»¶æ”¾å…¥ test_assets ç›®å½•")
        return
    
    video = str(test_videos[0])
    print(f"\nåˆ†æè§†é¢‘: {video}")
    
    result = analyzer.analyze(video)
    
    print(f"\nğŸ“Š åˆ†æç»“æœ:")
    print(f"   æ—¶é•¿: {result.duration:.1f}ç§’")
    print(f"   åˆ†è¾¨ç‡: {result.resolution[0]}x{result.resolution[1]}")
    print(f"   å¸§ç‡: {result.fps:.1f}")
    print(f"   å…³é”®å¸§: {len(result.keyframes)} ä¸ª")
    print(f"   ä¸»è¦æƒ…æ„Ÿ: {result.main_emotion.value}")
    print(f"   å»ºè®®é£æ ¼: {result.suggested_style}")
    
    if result.keywords:
        print(f"   å…³é”®è¯: {', '.join(result.keywords[:10])}")
    
    print(f"\nğŸ“ æ‘˜è¦:")
    print(f"   {result.summary[:200]}...")
    
    if result.script_suggestion:
        print(f"\nâœï¸ è„šæœ¬å»ºè®®:")
        print(result.script_suggestion)


if __name__ == '__main__':
    demo_analyze()
